{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from db_queries import get_location_metadata\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get BMI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3974133/3891060890.py:2: DtypeWarning: Columns (46,47,48,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  bmi_raw_df = pd.read_csv(BMI_filepath,dtype={'hh_id': str})\n"
     ]
    }
   ],
   "source": [
    "BMI_FILEPATH = '/mnt/team/integrated_analytics/pub/goalkeepers/goalkeepers_2024/data/bmi/bmi_data_outliered.csv'\n",
    "bmi_raw_df = pd.read_csv(BMI_filepath,dtype={'hh_id': str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BMI wealth factor is all NA :( \n",
    "# Gotta bring the wealth data in from the other team\n",
    "bmi_raw_df.wealth_factor.isna().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Wealth Data and transform to income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_income_from_asset_score(asset_df:pd.DataFrame,  asset_score_col= 'asset_score', year_df_col = 'year_start'):\n",
    "    #Get and clean income data\n",
    "    INCOME_FILEPATH = '/mnt/team/rapidresponse/pub/population/data/02-processed-data/cgf_bmi/income_distributions.parquet'\n",
    "    income_raw = pd.read_parquet(INCOME_FILEPATH)\n",
    "    income_raw = income_raw[['pop_percent', 'cdf', 'location_id', 'year_id']]\n",
    "    income_raw['location_id'] = income_raw.location_id.astype(int)\n",
    "    income_raw['year_id'] = income_raw.year_id.astype(int)\n",
    "\n",
    "    wdf = asset_df.copy()\n",
    "    \n",
    "    # We get which asset score is what percentile of that asset score for each nid\n",
    "    get_percentile = lambda x: x.rank() / len(x) \n",
    "\n",
    "    assert('percentile' not in wdf.columns)\n",
    "    assert('nid' in wdf.columns)\n",
    "    assert('location_id' in wdf.columns)\n",
    "    wdf['percentile'] = wdf.groupby(['nid'], group_keys=False)[asset_score_col].apply(get_percentile)\n",
    "\n",
    "    wdf = wdf.sort_values(['percentile'])\n",
    "    income_raw = income_raw.sort_values(['pop_percent'])\n",
    "\n",
    "    income_interp = income_raw.merge(\n",
    "        wdf[['location_id', year_df_col, 'percentile']].drop_duplicates().rename(columns={year_df_col:'year_id', 'percentile':'pop_percent'}),\n",
    "        on=['location_id', 'year_id', 'pop_percent'],\n",
    "        how='outer')\n",
    "\n",
    "    # We then interpolate to get the percentiles that aren't included in the income dataset\n",
    "    income_interp = income_interp.sort_values(['location_id', 'year_id', 'pop_percent'])\n",
    "    income_interp = income_interp.set_index(['location_id', 'year_id', 'pop_percent'])\n",
    "    income_interp['income_per_day'] = income_interp.groupby(level=[0, 1], group_keys=False)['cdf'].apply(lambda x: x.interpolate(method='linear', limit_area='inside'))\n",
    "    income_interp = income_interp.reset_index()\n",
    "\n",
    "    wdf = wdf.merge(income_interp, how='left', left_on=['location_id', 'year_start', 'percentile'],\n",
    "        right_on = ['location_id', 'year_id', 'pop_percent'])\n",
    "\n",
    "    # Only NAs should be because of newer surveys for which we don't have income distribution yet\n",
    "    assert((wdf.loc[wdf.income_per_day.isna()].year_id > 2020).all())\n",
    "    return wdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEALTH_FILEPATH = '/mnt/share/scratch/users/victorvt/cgfwealth_spatial/dhs_wealth_uncollapsed_again.parquet'\n",
    "loc_meta = get_location_metadata(release_id = 9, location_set_id = 35)\n",
    "\n",
    "wealth_raw = pd.read_parquet(WEALTH_FILEPATH)\n",
    "wealth_df = wealth_raw#[wealth_raw['point'] == 1]\n",
    "wealth_df = wealth_df.rename(columns = {'wealth_score':'asset_score'})\n",
    "wealth_df = wealth_df[['iso3', 'nid', 'psu', 'hh_id', 'year_start', 'asset_score']].drop_duplicates()\n",
    "# In the wealth team's dataset, sometimes there are multiple asset scores for a given household id.\n",
    "# Take away those NIDs\n",
    "bad_wealth = wealth_df.groupby(['nid', 'hh_id', 'year_start', 'psu',]).size()\n",
    "bad_nid_wealth = list(bad_wealth[bad_wealth.gt(1)].reset_index().nid.unique())\n",
    "bad_nid_wealth = bad_nid_wealth + [20315]\n",
    "wealth_df = wealth_df[~wealth_df.nid.isin(bad_nid_wealth)]\n",
    "# Make sure that by nid, psu and hh_id they all have the same lat and long\n",
    "#grouped = wealth_df.groupby(['nid', 'psu', 'hh_id'])\n",
    "#assert((grouped['lat'].nunique().lt(2) & grouped['long'].nunique().lt(2)).all())\n",
    "# Sometimes an nid has more than a year\n",
    "assert(wealth_df.groupby(['nid', 'hh_id', 'year_start', 'psu']).size().sort_values().max() == 1)\n",
    "wealth_df = wealth_df.merge(loc_meta[['location_id', 'local_id']], left_on ='iso3', right_on='local_id', how='left')\n",
    "assert(wealth_df.location_id.notna().all())\n",
    "wealth_df['year_start'] = wealth_df['year_start'].astype(int)\n",
    "wealth_df['nid'] = wealth_df['nid'].astype(int)\n",
    "\n",
    "wealth_df = get_income_from_asset_score(wealth_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Climate information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIMATE_FILEPATH = '/mnt/team/rapidresponse/pub/population/data/02-processed-data/cgf_bmi/bmi_climate.parquet'\n",
    "\n",
    "#clim_raw = pd.read_parquet(CLIMATE_FILEPATH)\n",
    "\n",
    "window_size_for_last_years_avg = 5\n",
    "\n",
    "climate_df = pd.read_parquet(CLIMATE_FILEPATH)\n",
    "climate_df = climate_df.rename(columns={'latnum':'lat', 'longnum':'long'})\n",
    "climate_df = climate_df.sort_values(['year', 'lat', 'long'])\n",
    "rolling_mean_window_f = lambda x: x.rolling(window=window_size_for_last_years_avg).mean()\n",
    "cumulative_mean_f = lambda x: x.shift().expanding().mean()\n",
    "climate_df['over30_cumavg'] = climate_df.groupby(['lat', 'long'])['over30'].transform(cumulative_mean_f)\n",
    "climate_df['temp_cumavg'] = climate_df.groupby(['lat', 'long'])['temp'].transform(cumulative_mean_f)\n",
    "climate_df['precip_cumavg'] = climate_df.groupby(['lat', 'long'])['precip'].transform(cumulative_mean_f)\n",
    "climate_df['over30_diff'] = climate_df['over30'] - climate_df['over30_cumavg']\n",
    "climate_df['temp_diff'] = climate_df['temp'] - climate_df['temp_cumavg']\n",
    "climate_cols = [col for col in climate_df.columns if col not in ['lat', 'long', 'year']]\n",
    "\n",
    "# I don't think it makes sense to do the \"Mean # of days over 30 in lifetime\" since we probably don't have enough years of info\n",
    "# And it probably only matters when you're young. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trim and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1547221\n",
      "1547221\n",
      "1547221\n",
      "1495037\n",
      "1495037\n",
      "1465676\n"
     ]
    }
   ],
   "source": [
    "# Trim to only NIDs in common between BMI and wealth data\n",
    "common_nids = set(bmi_raw_df['nid'].unique()).intersection(set(wealth_df['nid'].unique()))\n",
    "wealth_df = wealth_df[wealth_df['nid'].isin(common_nids)]\n",
    "bmi_df = bmi_raw_df[bmi_raw_df['nid'].isin(common_nids)].copy()\n",
    "\n",
    "# Drop unused columns\n",
    "wealth_df = wealth_df[['iso3', 'nid', 'psu', 'hh_id', 'asset_score', 'income_per_day' ]]\n",
    "bmi_df = bmi_df[['nid', 'psu', 'hh_id', 'latnum', 'longnum', 'sex_id', 'age_year', 'ihme_loc_id', 'year_start', 'year_end', 'urban',\n",
    "    'pweight', 'bmi', 'low_adult_bmi']]\n",
    "bmi_df = bmi_df.rename(columns = {'latnum':'lat', 'longnum':'long', 'hh_id':'old_hh_id'})\n",
    "wealth_df = wealth_df.rename(columns = {'hh_id':'old_hh_id'})\n",
    "\n",
    "# Try to make household id usable to merge on - \n",
    "# TODO: why is it so different between the datasets if it's the same source?\n",
    "bmi_df['hh_id'] = bmi_df['old_hh_id'].str.split(r'[_ ]').str[-1]\n",
    "wealth_df['hh_id'] = wealth_df['old_hh_id'].str.split(r'[_ ]').str[-1]\n",
    "\n",
    "merge_cols = ['nid', 'hh_id', 'psu']\n",
    "\n",
    "merged_df = bmi_df.merge(wealth_df, on=merge_cols, how='left')\n",
    "print(len(bmi_df))\n",
    "print(len(merged_df))\n",
    "\n",
    "print(len(merged_df))\n",
    "merged_df = merged_df[~merged_df.nid.isin(merged_df.loc[merged_df.income_per_day.isna()].nid.unique())]\n",
    "print(len(merged_df))\n",
    "merged_df = merged_df.drop(columns=['old_hh_id_y', 'old_hh_id_x'])\n",
    "merged_df = merged_df.merge(climate_df, left_on=['lat', 'long', 'year_start'],\n",
    "    right_on=['lat', 'long', 'year'], how='left')\n",
    "print(len(merged_df))\n",
    "assert(merged_df[merged_df.over30.isna()].year_start.gt(2016).all())\n",
    "merged_df = merged_df[merged_df.year_start < 2017]\n",
    "print(len(merged_df))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_ROOT = Path(\"/mnt/team/rapidresponse/pub/population/data/02-processed-data/cgf_bmi\")\n",
    "merged_df.to_parquet(OUT_ROOT / \"bmi_processed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
